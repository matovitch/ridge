<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The AI-tlantic ridge</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="the-ai-tlantic-ridge">The AI-tlantic ridge</h1>
<p>When the mathematician Bernard Dwork proved a first third of the Weil conjectures in 1960, the already famous Alexander Grothendieck was unphased. Grothendieck was looking for his "etale cohomology" convinced it was the way to prove the full set of conjectures. It would only take <a href="http://www.numdam.org/item/?id=PMIHES_1974__43__273_0">14 more years</a> and many more mathematicians to get there. Jean-Pierre Serre, who was working with Grothendieck at the time, qualified Dwork’s proof as “<a href="https://www.youtube.com/watch?v=pOv-ygSynRI&amp;t=1444s">magnificent but quite finicky</a>”.</p>
<p>Imagine you are looking to climb a peak. You can attempt a direct but perilous climb to the top like Dwork or follow a twisty valley convinced to find some gentle ridges leading you to this summit and many more beside it like Grothendieck. If you can be lucky with the hard path, it is often smarter to recognize you likely won’t succeed without the meandering necessary to discover intermediary ridges or tools.</p>
<p>If you ask ChatGPT “Can you write and run a Python program to count the number of r(s) in this very question?” it will succeed brilliantly. However, if you ask it to count the number of r(s) in this paragraph without writing or running a program, it will fail most of the time. Thankfully ChatGPT does not have to rewrite a python interpreter. Similarly, I've heard grateful mathematicians compare Grothendieck's <a href="https://en.wikipedia.org/wiki/S%C3%A9minaire_de_G%C3%A9om%C3%A9trie_Alg%C3%A9brique_du_Bois_Marie">SGA4</a> to a nice ready-to-use coffee machine.</p>
<p>This week, London-based company DeepMind <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/">announced the creation of a program able to solve 4 out of 6 problems from the 2024 International Mathematical Olympiad</a>. They have trained Google’s chatbot Gemini to formalize problems for a proof assistant software called Lean and paired it with solver similar to chess (or go) programs (like Leela Zero).</p>
<p>As a side note, and despite the initial analogy relating math with alpinism, I subscribe to the <a href="https://plato.stanford.edu/entries/formalism-mathematics/">formalist</a> philosophy of mathematics and view this activity as a syntactical game that can induce vivid visions and lead you to adopt misguided but useful <a href="https://plato.stanford.edu/entries/platonism-mathematics/">Platonist</a> beliefs. As such, I was quite happy to observe the link from this mathematical solver to classic games like chess and go.</p>
<p>In SF, the future, like the weather, looks often brighter than in London. Californians keep dreaming of larger computers to achieve “consciousness” and “superintelligence”. In Paris, the AI startup Mistral is calling its latest chatbot “<a href="https://mistral.ai/news/mistral-large-2407/">large enough</a>” with around 123 billion parameters. You could also argue that the pursuit of scaling laws is environmentally irresponsible or that we will need to <a href="https://situational-awareness.ai/lock-down-the-labs/">regulate these humongous computers as we do nuclear weapons</a>. But I am an optimist believing big monolithic models will soon reach a dead end. Superintelligence only has a marginal predictive advantage in a chaotic world, and I think we often fear it because we cannot imagine how different from us it may be.</p>
<p>Tighter integration with symbolic programs with e.g. <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">MCTS</a>, <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">RL</a>, <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> and <a href="https://en.wikipedia.org/wiki/Bayesian_programming">Bayesian programming</a>; the ability to create domain-specific languages and libraries; and the capacity to keep training for harder problems with synthetic data, then being able to translate the results back into natural language: this to me is the future we should aim for. And I believe DeepMind’s result shows we may not need many more <a href="https://en.wikipedia.org/wiki/FLOPS">FLOPS</a>.</p>
<p>Let’s follow the valley without following the Valley.</p>
</div>
</body>

</html>
